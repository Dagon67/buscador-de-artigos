[
  {
    "source": "arXiv",
    "results": [
      {
        "title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for\n  Open-Vocabulary Semantic Segmentation",
        "abstract": "Open-vocabulary semantic segmentation models associate vision and text to\nlabel pixels from an undefined set of classes using textual queries, providing\nversatile performance on novel datasets. However, large shifts between training\nand test domains degrade their performance, requiring fine-tuning for effective\nreal-world applications. We introduce Semantic Library Adaptation (SemLA), a\nnovel framework for training-free, test-time domain adaptation. SemLA leverages\na library of LoRA-based adapters indexed with CLIP embeddings, dynamically\nmerging the most relevant adapters based on proximity to the target domain in\nthe embedding space. This approach constructs an ad-hoc model tailored to each\nspecific input without additional training. Our method scales efficiently,\nenhances explainability by tracking adapter contributions, and inherently\nprotects data privacy, making it ideal for sensitive applications.\nComprehensive experiments on a 20-domain benchmark built over 10 standard\ndatasets demonstrate SemLA's superior adaptability and performance across\ndiverse settings, establishing a new standard in domain adaptation for\nopen-vocabulary semantic segmentation.",
        "url": "http://arxiv.org/abs/2503.21780v1",
        "published": "2025-03-27T17:59:58Z",
        "source": "arXiv"
      },
      {
        "title": "VideoMage: Multi-Subject and Motion Customization of Text-to-Video\n  Diffusion Models",
        "abstract": "Customized text-to-video generation aims to produce high-quality videos that\nincorporate user-specified subject identities or motion patterns. However,\nexisting methods mainly focus on personalizing a single concept, either subject\nidentity or motion pattern, limiting their effectiveness for multiple subjects\nwith the desired motion patterns. To tackle this challenge, we propose a\nunified framework VideoMage for video customization over both multiple subjects\nand their interactive motions. VideoMage employs subject and motion LoRAs to\ncapture personalized content from user-provided images and videos, along with\nan appearance-agnostic motion learning approach to disentangle motion patterns\nfrom visual appearance. Furthermore, we develop a spatial-temporal composition\nscheme to guide interactions among subjects within the desired motion patterns.\nExtensive experiments demonstrate that VideoMage outperforms existing methods,\ngenerating coherent, user-controlled videos with consistent subject identities\nand interactions.",
        "url": "http://arxiv.org/abs/2503.21781v1",
        "published": "2025-03-27T17:59:58Z",
        "source": "arXiv"
      },
      {
        "title": "Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model",
        "abstract": "Video understanding models often struggle with high computational\nrequirements, extensive parameter counts, and slow inference speed, making them\ninefficient for practical use. To tackle these challenges, we propose\nMobile-VideoGPT, an efficient multimodal framework designed to operate with\nfewer than a billion parameters. Unlike traditional video large multimodal\nmodels (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,\nefficient projectors, and a small language model (SLM), enabling real-time\nthroughput. To further improve efficiency, we present an Attention-Based Frame\nScoring mechanism to select the key-frames, along with an efficient token\nprojector that prunes redundant visual tokens and preserves essential\ncontextual cues. We evaluate our model across well-established six video\nunderstanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).\nOur results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per\nsecond while outperforming existing state-of-the-art 0.5B-parameter models by 6\npoints on average with 40% fewer parameters and more than 2x higher throughput.\nOur code and models are publicly available at:\nhttps://github.com/Amshaker/Mobile-VideoGPT.",
        "url": "http://arxiv.org/abs/2503.21782v1",
        "published": "2025-03-27T17:59:58Z",
        "source": "arXiv"
      },
      {
        "title": "X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction",
        "abstract": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
        "url": "http://arxiv.org/abs/2503.21779v1",
        "published": "2025-03-27T17:59:57Z",
        "source": "arXiv"
      },
      {
        "title": "Test-Time Visual In-Context Tuning",
        "abstract": "Visual in-context learning (VICL), as a new paradigm in computer vision,\nallows the model to rapidly adapt to various tasks with only a handful of\nprompts and examples. While effective, the existing VICL paradigm exhibits poor\ngeneralizability under distribution shifts. In this work, we propose test-time\nVisual In-Context Tuning (VICT), a method that can adapt VICL models on the fly\nwith a single test sample. Specifically, we flip the role between the task\nprompts and the test sample and use a cycle consistency loss to reconstruct the\noriginal task prompt output. Our key insight is that a model should be aware of\na new test distribution if it can successfully recover the original task\nprompts. Extensive experiments on six representative vision tasks ranging from\nhigh-level visual understanding to low-level image processing, with 15 common\ncorruptions, demonstrate that our VICT can improve the generalizability of VICL\nto unseen new domains. In addition, we show the potential of applying VICT for\nunseen tasks at test time. Code: https://github.com/Jiahao000/VICT.",
        "url": "http://arxiv.org/abs/2503.21777v1",
        "published": "2025-03-27T17:59:52Z",
        "source": "arXiv"
      },
      {
        "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
        "abstract": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.",
        "url": "http://arxiv.org/abs/2503.21776v1",
        "published": "2025-03-27T17:59:51Z",
        "source": "arXiv"
      },
      {
        "title": "Optimal Stepsize for Diffusion Sampling",
        "abstract": "Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.",
        "url": "http://arxiv.org/abs/2503.21774v1",
        "published": "2025-03-27T17:59:46Z",
        "source": "arXiv"
      },
      {
        "title": "StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross\n  Fusion",
        "abstract": "We present StyleMotif, a novel Stylized Motion Latent Diffusion model,\ngenerating motion conditioned on both content and style from multiple\nmodalities. Unlike existing approaches that either focus on generating diverse\nmotion content or transferring style from sequences, StyleMotif seamlessly\nsynthesizes motion across a wide range of content while incorporating stylistic\ncues from multi-modal inputs, including motion, text, image, video, and audio.\nTo achieve this, we introduce a style-content cross fusion mechanism and align\na style encoder with a pre-trained multi-modal model, ensuring that the\ngenerated motion accurately captures the reference style while preserving\nrealism. Extensive experiments demonstrate that our framework surpasses\nexisting methods in stylized motion generation and exhibits emergent\ncapabilities for multi-modal motion stylization, enabling more nuanced motion\nsynthesis. Source code and pre-trained models will be released upon acceptance.\nProject Page: https://stylemotif.github.io",
        "url": "http://arxiv.org/abs/2503.21775v1",
        "published": "2025-03-27T17:59:46Z",
        "source": "arXiv"
      },
      {
        "title": "LOCORE: Image Re-ranking with Long-Context Sequence Modeling",
        "abstract": "We introduce LOCORE, Long-Context Re-ranker, a model that takes as input\nlocal descriptors corresponding to an image query and a list of gallery images\nand outputs similarity scores between the query and each gallery image. This\nmodel is used for image retrieval, where typically a first ranking is performed\nwith an efficient similarity measure, and then a shortlist of top-ranked images\nis re-ranked based on a more fine-grained similarity measure. Compared to\nexisting methods that perform pair-wise similarity estimation with local\ndescriptors or list-wise re-ranking with global descriptors, LOCORE is the\nfirst method to perform list-wise re-ranking with local descriptors. To achieve\nthis, we leverage efficient long-context sequence models to effectively capture\nthe dependencies between query and gallery images at the local-descriptor\nlevel. During testing, we process long shortlists with a sliding window\nstrategy that is tailored to overcome the context size limitations of sequence\nmodels. Our approach achieves superior performance compared with other\nre-rankers on established image retrieval benchmarks of landmarks (ROxf and\nRPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)\nwhile having comparable latency to the pair-wise local descriptor re-rankers.",
        "url": "http://arxiv.org/abs/2503.21772v1",
        "published": "2025-03-27T17:59:44Z",
        "source": "arXiv"
      },
      {
        "title": "A Unified Image-Dense Annotation Generation Model for Underwater Scenes",
        "abstract": "Underwater dense prediction, especially depth estimation and semantic\nsegmentation, is crucial for gaining a comprehensive understanding of\nunderwater scenes. Nevertheless, high-quality and large-scale underwater\ndatasets with dense annotations remain scarce because of the complex\nenvironment and the exorbitant data collection costs. This paper proposes a\nunified Text-to-Image and DEnse annotation generation method (TIDE) for\nunderwater scenes. It relies solely on text as input to simultaneously generate\nrealistic underwater images and multiple highly consistent dense annotations.\nSpecifically, we unify the generation of text-to-image and text-to-dense\nannotations within a single model. The Implicit Layout Sharing mechanism (ILS)\nand cross-modal interaction method called Time Adaptive Normalization (TAN) are\nintroduced to jointly optimize the consistency between image and dense\nannotations. We synthesize a large-scale underwater dataset using TIDE to\nvalidate the effectiveness of our method in underwater dense prediction tasks.\nThe results demonstrate that our method effectively improves the performance of\nexisting underwater dense prediction models and mitigates the scarcity of\nunderwater data with dense annotations. We hope our method can offer new\nperspectives on alleviating data scarcity issues in other fields. The code is\navailable at https: //github.com/HongkLin/TIDE.",
        "url": "http://arxiv.org/abs/2503.21771v1",
        "published": "2025-03-27T17:59:43Z",
        "source": "arXiv"
      }
    ],
    "error": null
  },
  {
    "source": "PubMed",
    "results": [],
    "error": "No results found"
  },
  {
    "source": "OpenAlex",
    "results": [
      {
        "title": "3D Slicer as an image computing platform for the Quantitative Imaging Network",
        "abstract": null,
        "url": "https://doi.org/10.1016/j.mri.2012.05.001",
        "published": "2012-07-07",
        "source": "OpenAlex"
      },
      {
        "title": "A Metaverse: Taxonomy, Components, Applications, and Open Challenges",
        "abstract": null,
        "url": "https://doi.org/10.1109/access.2021.3140175",
        "published": "2022-01-01",
        "source": "OpenAlex"
      },
      {
        "title": "Personal Sensing: Understanding Mental Health Using Ubiquitous Sensors and Machine Learning",
        "abstract": null,
        "url": "https://doi.org/10.1146/annurev-clinpsy-032816-044949",
        "published": "2017-04-04",
        "source": "OpenAlex"
      },
      {
        "title": "Blended learning in higher education: Trends and capabilities",
        "abstract": null,
        "url": "https://doi.org/10.1007/s10639-019-09886-3",
        "published": "2019-02-22",
        "source": "OpenAlex"
      },
      {
        "title": "Machine and deep learning methods for radiomics",
        "abstract": null,
        "url": "https://doi.org/10.1002/mp.13678",
        "published": "2020-05-01",
        "source": "OpenAlex"
      },
      {
        "title": "Changing How Writing Is Taught",
        "abstract": null,
        "url": "https://doi.org/10.3102/0091732x18821125",
        "published": "2019-03-01",
        "source": "OpenAlex"
      },
      {
        "title": "Designing Effective Serious Games: Opportunities and Challenges for Research",
        "abstract": null,
        "url": "https://doi.org/10.3991/ijet.v5s3.1500",
        "published": "2010-11-22",
        "source": "OpenAlex"
      },
      {
        "title": "Tackling Climate Change with Machine Learning",
        "abstract": null,
        "url": "https://doi.org/10.1145/3485128",
        "published": "2022-02-07",
        "source": "OpenAlex"
      },
      {
        "title": "Autism: the micro-movement perspective",
        "abstract": null,
        "url": "https://doi.org/10.3389/fnint.2013.00032",
        "published": "2013-01-01",
        "source": "OpenAlex"
      },
      {
        "title": "Recovery from disorders of consciousness: mechanisms, prognosis and emerging therapies",
        "abstract": null,
        "url": "https://doi.org/10.1038/s41582-020-00428-x",
        "published": "2020-12-14",
        "source": "OpenAlex"
      }
    ],
    "error": null
  },
  {
    "source": "Semantic Scholar",
    "results": [],
    "error": null
  },
  {
    "source": "CrossRef",
    "results": [
      {
        "title": "Personalized Learning and Student Achievement",
        "abstract": null,
        "url": "https://doi.org/10.4324/9781003237136-4",
        "published": "2022",
        "source": "CrossRef"
      },
      {
        "title": "Personalized Learning Examples in Gifted Education",
        "abstract": null,
        "url": "https://doi.org/10.4324/9781003237136-12",
        "published": "2022",
        "source": "CrossRef"
      },
      {
        "title": "Gifted Education, Talent Development, and Personalized Learning",
        "abstract": null,
        "url": "https://doi.org/10.4324/9781003237136-3",
        "published": "2022",
        "source": "CrossRef"
      },
      {
        "title": "The Role of Technology in Personalized Learning",
        "abstract": null,
        "url": "https://doi.org/10.4324/9781003237136-13",
        "published": "2022",
        "source": "CrossRef"
      },
      {
        "title": "The Theory and History of Personalized Learning",
        "abstract": null,
        "url": "https://doi.org/10.4324/9781003237136-2",
        "published": "2022",
        "source": "CrossRef"
      },
      {
        "title": "Implementation of and Teacher Support for Personalized Learning",
        "abstract": null,
        "url": "https://doi.org/10.4324/9781003237136-14",
        "published": "2022",
        "source": "CrossRef"
      },
      {
        "title": "Inquiry Models of Learning",
        "abstract": null,
        "url": "https://doi.org/10.4324/9781003237136-8",
        "published": "2022",
        "source": "CrossRef"
      },
      {
        "title": "Temporal and adaptive processes of regulated learning - What can multimodal data tell?",
        "abstract": null,
        "url": "https://doi.org/10.1016/j.learninstruc.2019.101268",
        "published": "2021",
        "source": "CrossRef"
      },
      {
        "title": "Personalized Learning in Gifted Education",
        "abstract": null,
        "url": "https://doi.org/10.4324/9781003237136",
        "published": null,
        "source": "CrossRef"
      },
      {
        "title": "Personalized Talent Development Plans",
        "abstract": null,
        "url": "https://doi.org/10.4324/9781003237136-6",
        "published": "2022",
        "source": "CrossRef"
      }
    ],
    "error": null
  }
]