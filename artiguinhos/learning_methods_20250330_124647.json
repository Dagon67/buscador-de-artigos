[
  {
    "source": "arXiv",
    "results": [
      {
        "title": "VideoMage: Multi-Subject and Motion Customization of Text-to-Video\n  Diffusion Models",
        "abstract": "Customized text-to-video generation aims to produce high-quality videos that\nincorporate user-specified subject identities or motion patterns. However,\nexisting methods mainly focus on personalizing a single concept, either subject\nidentity or motion pattern, limiting their effectiveness for multiple subjects\nwith the desired motion patterns. To tackle this challenge, we propose a\nunified framework VideoMage for video customization over both multiple subjects\nand their interactive motions. VideoMage employs subject and motion LoRAs to\ncapture personalized content from user-provided images and videos, along with\nan appearance-agnostic motion learning approach to disentangle motion patterns\nfrom visual appearance. Furthermore, we develop a spatial-temporal composition\nscheme to guide interactions among subjects within the desired motion patterns.\nExtensive experiments demonstrate that VideoMage outperforms existing methods,\ngenerating coherent, user-controlled videos with consistent subject identities\nand interactions.",
        "url": "http://arxiv.org/abs/2503.21781v1",
        "published": "2025-03-27T17:59:58Z",
        "source": "arXiv"
      },
      {
        "title": "Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model",
        "abstract": "Video understanding models often struggle with high computational\nrequirements, extensive parameter counts, and slow inference speed, making them\ninefficient for practical use. To tackle these challenges, we propose\nMobile-VideoGPT, an efficient multimodal framework designed to operate with\nfewer than a billion parameters. Unlike traditional video large multimodal\nmodels (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,\nefficient projectors, and a small language model (SLM), enabling real-time\nthroughput. To further improve efficiency, we present an Attention-Based Frame\nScoring mechanism to select the key-frames, along with an efficient token\nprojector that prunes redundant visual tokens and preserves essential\ncontextual cues. We evaluate our model across well-established six video\nunderstanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).\nOur results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per\nsecond while outperforming existing state-of-the-art 0.5B-parameter models by 6\npoints on average with 40% fewer parameters and more than 2x higher throughput.\nOur code and models are publicly available at:\nhttps://github.com/Amshaker/Mobile-VideoGPT.",
        "url": "http://arxiv.org/abs/2503.21782v1",
        "published": "2025-03-27T17:59:58Z",
        "source": "arXiv"
      },
      {
        "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
        "abstract": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.",
        "url": "http://arxiv.org/abs/2503.21776v1",
        "published": "2025-03-27T17:59:51Z",
        "source": "arXiv"
      },
      {
        "title": "Near field imaging of local interference in radio interferometric data:\n  Impact on the redshifted 21-cm power spectrum",
        "abstract": "Radio-frequency interference (RFI) is a major systematic limitation in radio\nastronomy, particularly for science cases requiring high sensitivity, such as\n21-cm cosmology. Traditionally, RFI is dealt with by identifying its signature\nin the dynamic spectra of visibility data and flagging strongly affected\nregions. However, for RFI sources that do not occupy narrow regions in the\ntime-frequency space, such as persistent local RFI, modeling these sources\ncould be essential to mitigating their impact. This paper introduces two\nmethods for detecting and characterizing local RFI sources from radio\ninterferometric visibilities: matched filtering and maximum a posteriori (MAP)\nimaging. These algorithms use the spherical wave equation to construct\nthree-dimensional near-field image cubes of RFI intensity from the\nvisibilities. The matched filter algorithm can generate normalized maps by\ncross-correlating the expected contributions from RFI sources with the observed\nvisibilities, while the MAP method performs a regularized inversion of the\nvisibility equation in the near field. We also develop a full polarization\nsimulation framework for RFI and demonstrate the methods on simulated\nobservations of local RFI sources. The stability, speed, and errors introduced\nby these algorithms are investigated, and, as a demonstration, the algorithms\nare applied to a subset of NenuFAR observations to perform spatial, spectral,\nand temporal characterization of two local RFI sources. We assess the impact of\nlocal RFI on images, the uv plane, and cylindrical power spectra through\nsimulations and describe these effects qualitatively. We also quantify the\nlevel of errors and biases that these algorithms induce and assess their\nimplications for the estimated 21-cm power spectrum with radio interferometers.\nThe near-field imaging and simulation codes are made available publicly in the\nPython library nfis.",
        "url": "http://arxiv.org/abs/2503.21728v1",
        "published": "2025-03-27T17:40:38Z",
        "source": "arXiv"
      },
      {
        "title": "MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX",
        "abstract": "Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence.",
        "url": "http://arxiv.org/abs/2503.21699v1",
        "published": "2025-03-27T17:04:33Z",
        "source": "arXiv"
      },
      {
        "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks",
        "abstract": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.",
        "url": "http://arxiv.org/abs/2503.21696v1",
        "published": "2025-03-27T17:00:51Z",
        "source": "arXiv"
      },
      {
        "title": "Inverse Lax-Wendroff boundary treatment for solving conservation laws\n  with finite difference HWENO methods",
        "abstract": "This paper presents a novel inverse Lax-Wendroff (ILW) boundary treatment for\nfinite difference Hermite weighted essentially non-oscillatory (HWENO) schemes\nto solve hyperbolic conservation laws on arbitrary geometries. The complex\ngeometric domain is divided by a uniform Cartesian grid, resulting in challenge\nin boundary treatment. The proposed ILW boundary treatment could provide high\norder approximations of both solution values and spatial derivatives at ghost\npoints outside the computational domain. Distinct from existing ILW approaches,\nour boundary treatment constructs the extrapolation via optimized through a\nleast squares formulation, coupled with the spatial derivatives at the boundary\nobtained via the ILW procedure. Theoretical analysis indicates that compared\nwith other ILW methods, our proposed one would require fewer terms by using the\nrelatively complicated ILW procedure and thus improve computational efficiency\nwhile preserving accuracy and stability. The effectiveness and robustness of\nthe method are validated through numerical experiments.",
        "url": "http://arxiv.org/abs/2503.21626v1",
        "published": "2025-03-27T15:44:58Z",
        "source": "arXiv"
      },
      {
        "title": "Barrier-Free Microhabitats: Self-Organized Seclusion in Microbial\n  Communities",
        "abstract": "Bacteria frequently colonize natural microcavities such as gut crypts, plant\napoplasts, and soil pores. Recent studies have shown that the physical\nstructure of these spaces plays a crucial role in shaping the stability and\nresilience of microbial populations (Karita et al., PNAS 2022, Postek et al.\nPNAS 2024). Here, we demonstrate that protected microhabitats can emerge\ndynamically, even in the absence of physical barriers. Interactions with\nsurface features -- such as roughness or friction -- lead microbial populations\nto self-organize into effectively segregated subpopulations. Our numerical and\nanalytical models reveal that this self-organization persists even when strains\nhave different growth rates, allowing slower-growing strains to avoid\ncompetitive exclusion. These findings suggest that emergent spatial structuring\ncan serve as a fundamental mechanism for maintaining microbial diversity,\ndespite selection pressures, competition, and genetic drift.",
        "url": "http://arxiv.org/abs/2503.21621v1",
        "published": "2025-03-27T15:40:38Z",
        "source": "arXiv"
      },
      {
        "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
        "abstract": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.",
        "url": "http://arxiv.org/abs/2503.21620v1",
        "published": "2025-03-27T15:39:30Z",
        "source": "arXiv"
      },
      {
        "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
        "abstract": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.",
        "url": "http://arxiv.org/abs/2503.21614v1",
        "published": "2025-03-27T15:36:30Z",
        "source": "arXiv"
      }
    ],
    "error": null
  },
  {
    "source": "PubMed",
    "results": [],
    "error": "No results found"
  },
  {
    "source": "OpenAlex",
    "results": [],
    "error": null
  },
  {
    "source": "Semantic Scholar",
    "results": [],
    "error": null
  },
  {
    "source": "CrossRef",
    "results": [
      {
        "title": "The effect of Electronic stories on the level of visual-spatial intelligence for primary school pupils",
        "abstract": null,
        "url": "https://doi.org/10.21608/bsujpc.2022.240474",
        "published": "2022",
        "source": "CrossRef"
      },
      {
        "title": "Research Advances on Tailored Gamification",
        "abstract": null,
        "url": "https://doi.org/10.1007/978-981-32-9812-5_3",
        "published": "2019",
        "source": "CrossRef"
      },
      {
        "title": "Multimodal Literacies",
        "abstract": null,
        "url": "https://doi.org/10.2307/jj.26931997.10",
        "published": "2015",
        "source": "CrossRef"
      },
      {
        "title": "Sensory Literacies",
        "abstract": null,
        "url": "https://doi.org/10.2307/jj.26931997.13",
        "published": "2015",
        "source": "CrossRef"
      },
      {
        "title": "Efficacité de la stratégie de gamification pour développer des compétences de la fluidité linguistiques en français chez les étudiants du cycle secondaire",
        "abstract": null,
        "url": "https://doi.org/10.21608/bsujpc.2022.281215",
        "published": "2022",
        "source": "CrossRef"
      },
      {
        "title": "Spatial mnemonics",
        "abstract": null,
        "url": "https://doi.org/10.1201/b13410-24",
        "published": "2008",
        "source": "CrossRef"
      },
      {
        "title": "Socio-spatial Literacies",
        "abstract": null,
        "url": "https://doi.org/10.2307/jj.26931997.11",
        "published": "2015",
        "source": "CrossRef"
      },
      {
        "title": "Unlocking Educational Potential: Gamification Strategies in Developing Nation’s Science Curriculum",
        "abstract": null,
        "url": "https://doi.org/10.18848/2327-7963/cgp/v31i02/159-183",
        "published": "2024",
        "source": "CrossRef"
      },
      {
        "title": "4. Multimodal Literacies",
        "abstract": null,
        "url": "https://doi.org/10.21832/9781783094639-008",
        "published": "2015",
        "source": "CrossRef"
      },
      {
        "title": "7. Sensory Literacies",
        "abstract": null,
        "url": "https://doi.org/10.21832/9781783094639-011",
        "published": "2015",
        "source": "CrossRef"
      }
    ],
    "error": null
  }
]